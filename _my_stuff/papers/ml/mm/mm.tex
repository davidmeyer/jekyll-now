\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format


%\usepackage{draftwatermark}
% \SetWatermarkText{Confidential}
% \SetWatermarkScale{5}
% \SetWatermarkLightness {0.85} 
% \SetWatermarkColor[rgb]{0.7,0,0}


\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{color}


\title{Random Musings on a Sunday Morning\\
\normalsize{(extraordinarily basic stuff that's worth reflecting upon now and then)}}
\author{David Meyer \\ dmm@\{1-4-5.net,uoregon.edu\}}

 \date{13 Oct 2015}							% Activate to display a given date or no date


\begin{document}
\maketitle

\section{Introduction} 
\label{sec:intro}

One of the interesting things about machine learning is that it affords many different ways to look at the same data and hence underlying phenomena (aside: why exactly should this be?). For example, the Logistic Function is really a special case of a Conditional Random Field and PCA is a special case of a Linear AutoEncoder (with no regularization). This document looks at a different correspondence. Here well look at why  minimizing error (a sum), maximizing probability (a product), and minimizing energy (in an energy based model, for example,  a Restricted Boltzmann Machine) are all really the same thing. Note that this document is likely to have many errors.

%\newpage

\section{Minimizing Cost is a Sum}
\subsection{Linear Regression Cost Function}

For linear regression, our  \textit{hypothesis}  $h_{\theta}(\boldsymbol{x}) = g(\boldsymbol{\theta}^{T}\boldsymbol{x})$, where $g(z) = z$ and  $\boldsymbol{x}$ and $\boldsymbol{\theta}$ are $\text{N} \times 1$ column vectors (hence $\boldsymbol{\theta}^T$ is a $1 \times \text{N}$ row vector)\footnote{While $g(z)$  is linear here, in other models  $g$ can be a non-linearity such as the \textit{sigmoid} function.}:
\bigskip

\begin{flalign*}
\boldsymbol{x}             & = \begin{bmatrix}  x_{1}         \\  x_{2}         \\ \vdots \\  x_{n}          \end{bmatrix} \\ \\
\boldsymbol{\theta}      & = \begin{bmatrix}  \theta_{1}  \\  \theta_{2} \\  \vdots \\ \theta_{n}   \end{bmatrix}  \\ \\
\boldsymbol{\theta}^T  & = \Big [\theta_{1}, \theta_{2}, \cdots, \theta_{n}  \Big ]
\end{flalign*}

\bigskip
\noindent
which implies that
  
\begin{equation}
\label{eqn:dot_product}
\boldsymbol{\theta}^T\boldsymbol{x} = 
\Big [\theta_{1}, \theta_{2}, \cdots, \theta_{n}  \Big ]
\left[ \begin{array}{cccc} x_1 \\ x_2 \\ \vdots \\ x_n \end{array} \right] =
\sum\limits_{i = 1}^{n}\theta_{i}x_{i} = \theta_{1}x_1 + \theta_{2}x_2 +  \ldots + \theta_{n}x_n
\end{equation}
Note that Equation~\ref{eqn:dot_product} is the \emph{algebraic interpretation} of the dot product of $\boldsymbol{\theta}^T$ and $\boldsymbol{x}$ (also called the inner or scalar product). An alternate, \emph{geometric interpretation} of the dot product in Euclidian space, where a geometrical object possesses both a magnitude and a direction is

\begin{equation}
\label{eqn:geometric_dot_product}
\boldsymbol{\theta}^T \boldsymbol{x} = \|\boldsymbol{\theta}^T\| \|\boldsymbol{x}\| \cos(\alpha)
\end{equation}

\bigskip
\noindent
where $\alpha$ is the is the angle between the vectors $\boldsymbol{\theta}^T$ and  $\boldsymbol{x}$, and  $\|\boldsymbol{\theta}^T\|$ and $\|\boldsymbol{x}\|$ are the magnitudes of the vectors
$\boldsymbol{\theta}^T$ and  $\boldsymbol{x}$ respectively (that is, the $L^2$-norm\footnote{The general form of the  $p$-norm is $ \| x  \|_{p} = \bigg(\sum\limits_{i=1}^{n} |x_{i}|^{p} \bigg)^{\frac{1}{p}}$}).
Note that Equation \ref{eqn:geometric_dot_product} implies that 
\begin{equation}
\cos(\alpha) = \frac{\boldsymbol{\theta}^T \boldsymbol{x}}{ \|\boldsymbol{\theta}^T\| \|\boldsymbol{x}\|}
= \frac{\sum\limits_{i=1}^{n}\theta_{i}^{T} x_{i}}{\sqrt{\sum\limits_{i=1}^{n}(\theta_i^T)^2}\times \sqrt{\sum\limits_{i=1}^{n}(x_{i})^2}}
\end{equation}

\bigskip
\noindent
which is also known as the \emph{cosine similarity} between $\boldsymbol{\theta}^T$ and $\boldsymbol{x}$\cite{COSINE_SIMILARITY}.

\bigskip
\noindent
Getting back to our hypothesis, we see that  $h_{\theta}(\boldsymbol{x}) = g(\boldsymbol{\theta}^{T}\boldsymbol{x}) = \theta_0 + \theta_{1}x_1 + \theta_{2}x_2 +  \ldots + \theta_{n}x_n$. Note that by convention $\theta_{0}$ is treated specially; in particular, it is not part of the vector $\boldsymbol{\theta}$.

\noindent
Now, given this hypothesis, our cost function can be written as a \textit{sum}:

\begin{equation}
J(\boldsymbol{\theta}) = \frac{1}{n}\sum\limits_{i = 1}^{n}(h{_\theta}(x^{(i)})- y^{(i)} )^2
\label{eqn:simple_cost}
\end{equation}
The goal of machine learning then is to find the parameters $\boldsymbol{\theta}$ such that loss/error function $J(\boldsymbol{\theta})$ is minimized (note that which error is minimized, and when, is a topic unto itself).  For linear regression, Equation \ref{eqn:simple_cost} is a \textit{convex} optimization objective.
 
 \subsection{Logistic Regression Cost Function} 

For logistic regression, our hypothesis $h_{\theta}(\boldsymbol{x})$ is slightly different, as shown in Equations \ref{eqn:logistic_regression}, \ref{eqn:logistic_function} and \ref{eqn:combined_logistic}.

 \begin{equation}
 h_{\theta}(\boldsymbol{x}) = g(\boldsymbol{\theta}^{T}\boldsymbol{x})
 \label{eqn:logistic_regression}
 \end{equation}
 
 \bigskip
 \noindent
Here  $g(z)$ is the \textit{logistic} or \textit{sigmoid} function\footnote{BTW, the logistic function is a special case of a \textit{Conditional Random Field}}, and is defined as follows
 
 
 \begin{equation}
 g(z) = \frac{1}{1+e^{-z}}
 \label{eqn:logistic_function}
 \end{equation}
 Putting Equations \ref{eqn:logistic_regression} and \ref{eqn:logistic_function} together we get

 \begin{equation}
  h_{\theta}(\boldsymbol{x}) = g(\boldsymbol{\theta}^{T}\boldsymbol{x}) = 
  \frac{1}{1+e^{-\boldsymbol{\theta}^{T}\boldsymbol{x}}}
 \label{eqn:combined_logistic}
 \end{equation}
While it seems like we could perhaps use the same loss function as we did in linear regression (Equation~\ref{eqn:simple_cost}), it turns out that this loss function is \textit{non-convex} when applied to logistic regression, i.e., when $g(z) = \frac{1}{1 +  e^{-z}}$. As as result we typically use some version of \textit{cross-entropy} as the loss function:
 
 \begin{equation}
 J(\boldsymbol{\theta}) = \frac{1}{n} \Bigg [ \sum\limits_{i = 1}^{n}
 y^{(i)} \log h_{\theta}(x^{(i)}) +
 (1 - y^{(i)}) \log (1 - h_{\theta}(x^{(i))})) \Bigg]
\label{eqn:logistic_regression_cost}
\end{equation}

\bigskip
\noindent
The regularized version of Equation \ref{eqn:logistic_regression_cost} is

\begin{equation}
\label{eqn:normalized_lr}
J(\boldsymbol{\theta}) = \frac{1}{n} \Bigg [ \sum\limits_{i = 1}^{n}
 y^{(i)} \log h_{\theta}(x^{(i)}) +
 (1 - y^{(i)}) \log (1 - h_{\theta}(x^{(i))})) \Bigg] +
 \frac{\lambda}{2n} \sum\limits_{j = 1}^{n}\theta_{j}^{2}
\end{equation}
\bigskip

\subsection{Deriving the Loss/Error function for Logistic Regression}

Recall that this loss function linear regression was
 
\begin{equation}
J(\theta) = \frac{1}{n}\sum\limits_{i = 1}^{n}
(h_{\theta}(x^{(i)}) - y^{(i)})^2
\end{equation}
and that this loss function was a \emph{non-convex} optimization objective for logistic regression. To find a convex loss function for logistic regression we first define a $Cost$ function:

\begin{equation}
Cost(h_{\theta}(x^{(i)}),y^{(i)}) = (h_{\theta}(x^{(i)}) - y^{(i)})^2
\end{equation}

\bigskip
\noindent
so that 

\begin{equation}
J(\boldsymbol{\theta}) = \frac{1}{n} \sum\limits_{i = 1}^{n} Cost(h_{\theta}(x^{(i)}),y^{(i)}) 
\end{equation}

\bigskip
\noindent
Now we can ask what our $Cost$ function look like.  Well, if we predict $1$ and $y = 1$, the the cost should be close to $0$ (because we predicted the right value). Alternatively, if we predict $0$ and $y = 1$, then the cost should converge on $\infty$ (that is, we penalize the prediction). On the other hand, if we predict $0$ and $y =0$, the the cost should again be close to close to $0$ as we predicted the right value. Similarly, if we predict $1$ and $y =$, the cost should again converge on $\infty$. These two cases are captured in Equation \ref{eqn:cost}.

\begin{equation}
\label{eqn:cost}
Cost(h_\theta(x), y) =
\begin{cases}
 -\log (h_\theta(x))    & \textrm{if }  y=1\\
 -\log (1-h_\theta(x)) & \textrm{if } y=0
 \end{cases}
\end{equation}

\bigskip
\noindent
Note that if you combine the two cases in Equation \ref{eqn:cost} together, you get Equation \ref{eqn:logistic_regression_cost}, the \emph{cross entropy}.

\bigskip
\noindent
In any event, in both cases we fit the parameters $\boldsymbol{\theta}$ to the model by minimizing a \textit{\bf{sum}} such as  Equation \ref{eqn:simple_cost} or \ref{eqn:logistic_regression_cost}.

\section{Maximizing Probability is a Product}

Basic assumption: all of this analysis depends on the assumption, as you will see, that the error $\epsilon^{(i)}$ is Gaussian, so Danger Will Robinson!\footnote{In reality it could be any parametric distribution. In this case I know what the simplifying assumptions are/mean so I can write the density down.} That said, assume that the target variables and the inputs are related via the equation

\begin{equation}
y^{(i)} = \theta^{T}x^{(i)} + \epsilon^{(i)}
\end{equation}
and thus
\begin{equation}
e^{(i)} = y^{(i)} - \theta^{T}x^{(i)}
\end{equation}


\bigskip
\noindent
where $\epsilon^{(i)}$ is an error term that captures either unmodeled effects (such as
if there are some important features that we left out of the model, or just random noise). Interestingly this is intuitive as $\epsilon^{(i)} = y^{(i)} - \theta^{T} x^{(i)}$.  Assume also 
that the $\epsilon^{(i)}$'s are IID (Independent and Identically Distributed) according to some Gaussian distribution with mean $\mu = 0$ and variance $\sigma^2$, i.e., $\mathcal{N}(0,\sigma^2)$ (note that this argument also relies on the \emph{Central Limit Theorem} and the \emph{Law of Large Numbers} \cite{LLN}). We typically use the notation $X \sim \mathcal{N}(\mu,\sigma^2)$ to indicate that the random variable $X$ is normally distributed with mean $\mu$ and variance  $\sigma^2$ (note that if $X \sim \mathcal{N}(0,1)$ we say that $X$ follows the \emph{standard normal} distribution).  

\bigskip
\noindent
The general form of the probability density function (pdf) of $\mathcal{N}(\mu,\sigma^2)$ is\footnote{$exp(x)$ is defined to be $e^x$}


\begin{equation}
P(x) = f(x \mid \mu, \sigma) = \frac{1}{\sqrt[]{2 \pi \sigma}}exp\Bigg[-\frac{(x-\mu)^2}{2\sigma^2}\Bigg],  \quad - \infty  < x < \infty
\end{equation}

\bigskip
\noindent
Here $\mu = \mathbb{E}[X]$, i.e., the mean (and mode), $\sigma^2 = \textrm{var}[X]$, and
 $\sqrt{2 \pi \sigma^2}$ is a normalization constant that ensures that the density $f$ integrates to 1.

\bigskip
\noindent
Now, assuming $e^{(i)} \sim \mathcal{N}(0,\sigma^2)$, we can write the density of $\epsilon^{(i)}$ as\footnote{We can always "normalize" our data so that $\mu = 0$ by computing the $z$ score, $z =
\frac{x - \mu}{\sigma}$}:

\begin{equation}
p(\epsilon^{(i)}) = \frac{1}{\sqrt[]{2 \pi \sigma}}exp\Bigg[-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\Bigg]
\label{eqn:density}
\end{equation}
so that

\begin{equation}
p(y^{(i)} \mid x^{(i)}; \theta) = \frac{1}{\sqrt[]{2 \pi \sigma}}exp\Bigg[-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^2}\Bigg]
\label{eqn:prob_lg}
\end{equation}

\bigskip
\noindent
Rewriting this in vector/sum notation

\begin{equation}
p(\bold{y} | \bold{X}, \boldsymbol{\theta},\sigma) = (2\pi\sigma^{2})^{-n/2}exp\Bigg[{-\frac{1}{2\sigma^{2}}\sum\limits_{i = 1}^{n} (y^{(i)} - \theta^{T}x^{(i)})^2\Bigg]}
\end{equation}
and taking advantage of the fact that  the sum of exponential powers is product of exponentials\footnote{$a^{(b+c)} = a^{b} \times a^{c}$} we get

\begin{equation}
p(\bold{y} | \bold{X}, \boldsymbol{\theta},\sigma) =  \prod \limits _{i = 1}^{n}(2\pi\sigma^{2})^{-1/2} exp \Bigg[{-\frac{1}{2\sigma^{2}}(y^{(i)} - \theta^{T}x^{(i)})^2\Bigg]}
\label{eqn:likelyhood}
\end{equation}

\bigskip
\noindent
Note that the \textit{likelyhood} of a set of parameter values $\theta$  given outcomes $x$ is equal to the probability of those observed outcomes given those parameter values, that is
\begin{equation}
\mathcal{L}(\theta \mid x) = P(x \mid \theta)
\end{equation}
In the discrete case
\begin{equation}
\mathcal{L}(\theta \mid x) = p_{\theta}(x) = P_{\theta}(X = x)
\end{equation}
Thus we can see that Equation~\ref{eqn:likelyhood} is essentially the likelihood of $\theta$ given $X$, and can be rewritten as 

\begin{equation}
\mathcal{L}(\theta) = \prod \limits _{i = 1}^{n} p(y^{(i)} \mid x^{(i)}; \theta)
\end{equation}
which familiarly implies 

\begin{equation}
\mathcal{L}(\theta) = \prod \limits _{i = 1}^{n} 
\frac{1}{\sqrt[]{2 \pi \sigma}}
exp \Bigg[-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^2}\Bigg]
\end{equation}
That is, maximizing the likelihood is a \textbf{product}. Going the other way, maximizing the log likelihood 
$\ell(\theta)$ gives

\begin{flalign}
\ell(\theta) &= \log \mathcal{L}(\theta)  \\
& = \log \prod \limits_{i = 1}^{n}
\frac{1}{\sqrt{2\pi}\sigma} 
exp \Bigg[-\frac{(y^{(i)}- \theta^{T} x^{(i)})^2} 
{2\sigma^{2}}\Bigg] \\
& = \sum\limits_{i = 1}^{n} \log
\frac{1}{\sqrt{2\pi}\sigma} 
exp \Bigg[-\frac{(y^{(i)}- \theta^{T} x^{(i)})^2} 
{2\sigma^{2}}\Bigg] \\
& = n \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2} 
\sum\limits_{i = 1}^{n}(y^{(i)} - \theta^{T}x^{(i)})^2
\end{flalign}

\noindent
Hence maximizing $\ell(\theta)$ gives the same answer as minimizing
\begin{equation}
\frac{1}{2}\sum\limits_{i=1}^{n}  (y^{(i)} - \theta^{T}x^{(i)})^2
\end{equation}
which turns out to be the original least squares cost function $J(\boldsymbol{\theta})$ (Equation \ref{eqn:simple_cost}).


\section{Minimizing Energy}
So we've seen that minimizing error (a sum) is roughly equivalent to maximizing the probability (or likelihood), a product. It turns out that minimizing energy in a physical system such as a system of springs is the same thing! In this section we'll look at minimizing energy functions such as used by Restricted Boltzmann Machines (RBMs).  Energy-based probabilistic models (e.g., RBMs) define a probability distribution through an energy function, as follows:

\begin{equation}
p(x) = \frac{e^{-E(x)}}{Z}
\end{equation}
where the normalizing factor $Z$, called a partition function and is  typically computationally intractable. $Z$ is  defined as follows
\begin{equation}
Z = \sum\limits_{i = 1}{}e^{-E(x)}
\end{equation}

\noindent
Using these definitions we can write the expression for the \emph{likelihood} $\mathcal{L}$ of $\theta$ and $\mathcal{D}$ as

\begin{flalign}
\mathcal{L}(\theta,\mathcal{D}) = \frac{1}{N}\sum\limits_{x^{(i)} \in \mathcal{D}}^{} \log p(x^{(i)}) \\
\ell(\theta,\mathcal{D}) = -\mathcal{L}(\theta,\mathcal{D})
\end{flalign}
which we can minimize using stochastic gradient descent (SGD) where the gradient is $-\frac{\partial \log p(x^{(i)}} {\partial \theta}$ and  $\boldsymbol{\theta}$ are the model parameters.

\newpage
\bibliographystyle{plain}
\bibliography{/Users/dmm/papers/bib/ml}



\end{document} 
