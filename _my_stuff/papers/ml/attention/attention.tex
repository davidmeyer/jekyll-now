\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format


%\usepackage{draftwatermark}
% \SetWatermarkText{Confidential}
% \SetWatermarkScale{5}
% \SetWatermarkLightness {0.85} 
% \SetWatermarkColor[rgb]{0.7,0,0}


\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{color}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\title{Notes on Attention Models for Neural Networks}
\author{David Meyer \\ dmm@\{1-4-5.net,uoregon.edu\}}

% \date{}							% Activate to display a given date or no date


\begin{document}
\maketitle

\section{Introduction} 
\label{sec:intro}

Statistical language models are probability distributions which have many applications, including speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, and information retrieval. Given a \emph{sequence} of  length $n$, a language model assigns a probability $p(x_{1},\ldots ,x_{n})$ to the whole sequence.  Here $x_i$ is the $i^{th}$ word or symbol in the sequence.  A  model that computes either $p(x_{1},\ldots ,x_{m})$ or $p(x_{n} | x_1, x_2, \cdots, x_{n-1})$ is called a language model. Using the chain rule for conditional probabilities, we can see that 

\begin{flalign}
\label{eqn:chain_rule}
p(x_{1}, x_{2}, \ldots, x_{n}) &= p(x_{1}) p(x_{2} | x_{1}) p(x_{3} | x_{1}, x_{2}) \cdots p(x_{n} | x_{1}, \ldots,  x_{n - 1}) \\
\label{eqn:chain_rule_prod}
&= \prod\limits_{i = 1}^{n} p(x_{i} | x_{1}, x_{2}, \ldots, x_{i - 1})
\end{flalign}


\bigskip
\noindent
Observe that since our language model is sequential\footnote{$x_i$ occurs before $x_j$ $\forall {i,j} \; 1 \leq i < j$}, the value of $n$ in Equation \ref{eqn:chain_rule_prod} is in some sense how far back in time we need to look to predict the next word  or symbol (I'll just say word from here on out). A language model that looks at the $n$ previous words in a sequence is called an \emph{n-gram}, and is defined using the chain rule as shown in Equation \ref{eqn:chain_rule}. Interesting \emph{n-gram} models include

\begin{flalign}
\label{eqn:unigram}
p(x_{1},x_{2},\ldots,x_{n}) &\approx \prod\limits_{i = 1}^{n} p(x_{i}) \qquad \mathbin{\#} \emph{unigram model} \\
p(x_{i} | x_{1},x_{2},\ldots, x_{i -1}) &\approx p(x_{i} |  x_{i-1}) \quad \mathbin{\#} \emph{bigram model }
\label{eqn:unigram}\\
\end{flalign}

\bigskip
\noindent
The unigram model assumes we can predict the next word independent of all of the words 
that came before. The bigram model assumes that the future is independent of the past given the present, i.e.,  the Markov assumption.\footnote{Sometimes called a \emph{first-order} Markov assumption.}

\bigskip
\noindent
Note that we can estimate the \emph{n-gram} probabilities in a straightforward way. Consider the \emph{bigram} case. Here the Maximum Likelihood Estimate, or MLE is simply

\begin{flalign}
\label{eqn:bigram_mle}
p(x_{i} |x_{i-1}) &= \frac{count(x_{i-i}, x_{i})}{count(x_{i-1})} \qquad \mathbin{\#} \emph{bigram MLE estimate}
\end{flalign}

\bigskip
\noindent
Here we are simply counting how many times $x_{i}$ appeared in the context $x_{i-1}$ and normalizing by all observations of $x_{i-1}$.

\bigskip
\noindent
There are a few problems with \emph{n-gram} models. First, for any reasonable $n$ the \emph{n-gram} is likely to be an insufficient model of the language due to the long-range typically found in natural language.  This forces larger values of $n$. The second is that \emph{n-grams} suffer from 
sparse data distributions; as $n$ grows  the space of all possible sequences grows rapidly and the probability of most sequences or next 
words is tends towards zero. In addition, the number of possible parameters grows exponentially with $n$. As a result, there will be never enough of the training data to estimate parameters of high-order \emph{n-gram} models. That said, there are many cases in which we can get away with \emph{n-grams}.

\section{Encoder-Decoder Architecture}
\noindent
Recurrent Neural Networks (RNNs) take a different approach to machine translation. Rather than keeping counts, a RNN 
summarizes what it has seen previous steps in  its current hidden state; you can think of the hidden state $h_t$  as the memory of the network. Thus $h_t$  captures information about what happened in the previous $t-1$ time steps. This means that the RNN must be able to summarize all the information from the $t-1$ previous steps in a \emph{fixed length} vector ($h_t$). This property will turn out to be one of the limitations of RNNs that motivated the development of attention mechanisms. Finally, note that the output distribution at time $t$, $y_t$, s calculated solely based on $h_t$. 

\bigskip
\noindent
As an aside, while a vanilla RNNs can in principle capture dependencies over some period of time in past, in practice they have trouble with long-range dependencies. As a result,  these networks are typically outfitted with some kind of memory, such as in the Long Short-Term Memory (LSTM) or the gated units described in \cite{Cho:2014aa}. Note here: Neural Turing Machines \cite{Graves:2014aa} and Differentiable Neural Computers 
\cite{Graves:2016aa} use a more explicit and function memory;  however, as pointed out in \cite{Daniluk:2017aa} the hidden state matrix is just a memory matrix of the form $[h_{t-L}\ldots h_{t-1}] \in \mathbb{R}^{n \times L}$, where $n$ is the output dimension of the RNN cells and $L$ is a sliding window. 


\bigskip
\noindent
Recent state of the art performance for translation tasks has been achieved using the Encoder-Decoder framework described in Sutskever et. al.\cite{NIPS2014_5346} and Cho et.al.\cite{Cho:2014aa}  and represents  alternate approach to language modeling. The following description of the Encoder-Decoder framework follows the notation found in \cite{Bahdanau:2014aa}. 

\bigskip
\noindent
In the Encoder-Decoder framework,  an encoder reads the input sentence, which is a sequence of vectors $\mathbf{x} = (x_1, \cdots, x_{T_x})$. 
The input sequence is taken from a vocabulary $V_x$, with $|V_x| = K_x$. Here each $x_i$ is a column vector of length $K_x$ such that $x_i \in \mathbb{R}^{K_{x} \times 1}$  (usually written $x_i \in \mathbb{R}^{K_x}$) is the one-hot encoding for the word at position $i$. That is,  the $i^{th}$ input word looks like

\begin{flalign*}
x_i = \begin{bmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{K_{x}i} 
\end{bmatrix}
\end{flalign*}

\bigskip
\noindent
so that

\bigskip

\begin{flalign*}
\mathbf{x} = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1i} & \cdots & x_{1T_x} \\
x_{21} & x_{22} & \cdots & x_{2i} & \cdots & x_{2T_x}\\
\vdots & \vdots & \ddots & \vdots & \ddots  & \vdots\\
x_{K_{x}1} & x_{K_{x}2} & \cdots & x_{K_{x}i} & \cdots &  x_{K_{x}T_x} \\
\end{bmatrix} 
\end{flalign*}

\bigskip
\noindent
The encoder RNN calculates its hidden state\footnote{In general the hidden state of the encoder is called $h_t$ and the hidden 
state of the decoder is called $s_t$.} at time $t$,  $h_t \in \mathbb{R}^n$  as
\begin{flalign}
\label{eqn:h_sub_t}
h_t = f(x_{t},h_{t-1})
\end{flalign}

\noindent
and also produces a context vector $c$ from the hidden states:  $c = q(\{h_{1},h_{2},\ldots, h_{T_x}\})$.  As an example,  \cite{NIPS2014_5346} used an LSTM for $f$ and defined $q(\{h_{1},h_{2},\ldots, h_{T}\}) = h_{T}$.

\bigskip
\noindent
The decoder is usually trained to predict the next word $y_t$ given the context vector $c$ and all the previously predicted words
$\{y_1, \ldots, y_{t-1}\}$; the decoder defines a probability over $\mathbf{y}$ (the translation) by decomposing it into its joint probabilities,
conditioned on the previously translated words and the context vector. 

\begin{flalign}
p(\mathbf{y}) = \prod\limits_{t = 1}^{T} p(y_{t} |\{ y_{1}, y_{2}, \ldots, y_{T_y}\},c)
\end{flalign}

\bigskip
\noindent
where $\mathbf{y} = (y_{1},\ldots,y_{T_y})$. With an RNN, the conditional probability of each translation is modeled as 
\begin{flalign}
p(y_{t} |\{ y_{1}, y_{2}, \ldots, y_{T_y}\},c) = g(y_{t-1}, s_t,c)
\end{flalign}

\bigskip
\noindent
where $g$ is a nonlinear, potentially multi-layered, function that outputs the probability of $y_t$, and $s_t$ is the hidden state of the RNN.


Recurrent Neural Networks (RNNs) take a different approach to machine translation.  From a probabilistic perspective, the machine translation task is to find a target sentence \textbf{y} that maximizes the conditional probability of \textbf{y} given a source sentence \textbf{x}, that is,  $\argmax\limits_{y} p(\mathbf{y} | \mathbf{x})$. In neural machine translation, a parameterized model is fit which maximizes the conditional probability of sentence pairs using a parallel training corpus. 

\subsection{Preliminaries}
\noindent
We consider an input sequence over a vocabulary $V_x$, with $|V_x| = K_x$,  where the input is a sequence of vectors $\mathbf{x} = (x_1, \cdots, x_{T_x})$. Here each $x_i$ is a column vector of length $K_x$ such that $x_i \in \mathbb{R}^{K_{x} \times 1}$  (usually written $x_i \in \mathbb{R}^{K_x}$) is the one-hot encoding for the word at position $i$. That is,  the $i^{th}$ input word looks like
\bigskip

$x_i = \begin{bmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{K_{x}i} 
\end{bmatrix}$

\bigskip
\noindent
so that
\bigskip


$\mathbf{x} = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1i} & \cdots & x_{1T_x} \\
x_{21} & x_{22} & \cdots & x_{2i} & \cdots & x_{2T_x}\\
\vdots & \vdots & \ddots & \vdots & \ddots  & \vdots\\
x_{K_{x}1} & x_{K_{x}2} & \cdots & x_{K_{x}i} & \cdots &  x_{K_{x}T_x}
\end{bmatrix}
$

\section{RNN Encode-Decoder Architecture} 
\label{sec:rnn}



\newpage
\bibliographystyle{plain}
\bibliography{/Users/dmm/papers/bib/ml}



\end{document} 
