\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Kingma:2013aa}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review: Latent Variable Models}{1}{section.2}}
\newlabel{eqn:sum_rule}{{1}{1}{Review: Latent Variable Models}{equation.2.1}{}}
\newlabel{eqn:product_rule}{{2}{1}{Review: Latent Variable Models}{equation.2.2}{}}
\newlabel{eqn:bayes}{{7}{2}{Review: Latent Variable Models}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Latent Variables?}{2}{subsection.2.1}}
\newlabel{eqn:model}{{12}{2}{Latent Variables?}{equation.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Problem of Approximate Inference}{3}{section.3}}
\newlabel{sec:approximate_inference}{{3}{3}{The Problem of Approximate Inference}{section.3}{}}
\newlabel{eqn:marginal_distribution}{{13}{3}{The Problem of Approximate Inference}{equation.3.13}{}}
\newlabel{eqn:marginal}{{14}{3}{The Problem of Approximate Inference}{equation.3.14}{}}
\newlabel{eqn:generative}{{17}{4}{The Problem of Approximate Inference}{equation.3.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Variational Autoencoders}{4}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Jensen's Inequality (image courtesy Wikipedia)}}{5}{figure.1}}
\newlabel{fig:jensens}{{1}{5}{Jensen's Inequality (image courtesy Wikipedia)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Choosing the Latent Variables $z$}{5}{subsection.4.1}}
\newlabel{eqn:gaussian}{{18}{5}{Choosing the Latent Variables $z$}{equation.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The standard VAE (directed) graphical model}}{6}{figure.2}}
\newlabel{fig:vaegm}{{2}{6}{The standard VAE (directed) graphical model}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The VAE (directed) Inference/Learning Challenge. Image courtesy \url  {http://videolectures.net/deeplearning2015_courville_autoencoder_extension/}}}{7}{figure.3}}
\newlabel{fig:vae_directed}{{3}{7}{The VAE (directed) Inference/Learning Challenge. Image courtesy \url {http://videolectures.net/deeplearning2015_courville_autoencoder_extension/}}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}VAE Objective Function}{7}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}So where do the $\mathbf  {z}$'s come from?}{7}{subsection.5.1}}
\newlabel{eqn:kl}{{19}{7}{So where do the $\mathbf {z}$'s come from?}{equation.5.19}{}}
\newlabel{eqn:kl1}{{20}{8}{So where do the $\mathbf {z}$'s come from?}{equation.5.20}{}}
\newlabel{eqn:kl2}{{21}{8}{So where do the $\mathbf {z}$'s come from?}{equation.5.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Neural networks for $q_{\phi }(z|x)$ and $p_{\theta }(x|z)$}}{9}{figure.4}}
\newlabel{fig:vae_parameter}{{4}{9}{Neural networks for $q_{\phi }(z|x)$ and $p_{\theta }(x|z)$}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}VAE Inference Model}{9}{section.6}}
\newlabel{eqn:vae_objective}{{25}{9}{VAE Inference Model}{equation.6.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}One Important "Trick"}{9}{section.7}}
\citation{Kingma:2013aa}
\bibstyle{plain}
\bibdata{/Users/dmm/papers/bib/vae.bib}
\bibcite{Kingma:2013aa}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Reparameterization Trick}}{10}{figure.5}}
\newlabel{fig:vae_reparam_trick}{{5}{10}{Reparameterization Trick}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}A Few Final Thoughts}{10}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgements}{10}{section.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A training-time variational autoencoder implemented as a feed-forward neural network, where $p_{\theta }(\mathbf  {x}|\mathbf  {z})$ is Gaussian. The network on the left is without the \emph  {reparameterization trick}, and network on the right is with it. Red shows sampling operations that are non-differentiable. Blue shows loss layers. The feedforward behavior of these networks is identical, but back propagation can be applied only to the right network. Image courtesy \url  {https://arxiv.org/abs/1606.05908}}}{11}{figure.6}}
\newlabel{fig:training_time_vae}{{6}{11}{A training-time variational autoencoder implemented as a feed-forward neural network, where $p_{\theta }(\mathbf {x}|\mathbf {z})$ is Gaussian. The network on the left is without the \emph {reparameterization trick}, and network on the right is with it. Red shows sampling operations that are non-differentiable. Blue shows loss layers. The feedforward behavior of these networks is identical, but back propagation can be applied only to the right network. Image courtesy \url {https://arxiv.org/abs/1606.05908}}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training the VAE via Backpropagation}}{12}{figure.7}}
\newlabel{fig:vae_training}{{7}{12}{Training the VAE via Backpropagation}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces VAE Graphical Model With No Encoder Pathway}}{12}{figure.8}}
\newlabel{fig:vae_no_encoder}{{8}{12}{VAE Graphical Model With No Encoder Pathway}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Appendix}{13}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Strong Law of Large Numbers}{13}{subsection.10.1}}
\newlabel{sec:slln}{{10.1}{13}{Strong Law of Large Numbers}{subsection.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Ergodic Theorem}{13}{subsection.10.2}}
\newlabel{sec:ergodic}{{10.2}{13}{Ergodic Theorem}{subsection.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cost Function from "Auto-encoding Variational Bayes"}}{14}{figure.9}}
\newlabel{fig:vae_cost}{{9}{14}{Cost Function from "Auto-encoding Variational Bayes"}{figure.9}{}}
